
@ONLINE {python:2015,
    author = "Python",
    title  = "Python Software Foundation",
    year   = "2015",
    url    = "https://www.python.org/"
}

@article{DeMillo:1978,
abstract = {In many cases tests of a program that uncover simple errors are also effective in uncovering much more complex errors. This so-called coupling effect can be used to save work during the testing process.},
author = {DeMillo, Richard a. and Lipton, Richard J. and Sayward, Frederick G.},
doi = {10.1109/C-M.1978.218136},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer},
number = {4},
pages = {34--41},
title = {{Hints on Test Data Selection: Help for the Practicing Programmer.}},
volume = {11},
year = {1978}
}

@article{Derezinska:2014,
abstract = {Mutation testing of Python programs raises a problem of incompetent mutants. Incompetent mutants cause execution errors due to inconsistency of types that cannot be resolved before run-time. We present a practical approach in which incompetent mutants can be generated, but the solution is transparent for a user and incompetent mutants are detected by a mutation system during test execution. Experiments with 20 traditional and object-oriented operators confirmed that the overhead can be accepted. The paper presents an experimental evaluation of the first- and higher-order mutation. Four algorithms to the 2nd and 3rd order mutant generation were applied. The impact of code coverage consideration on the process efficiency is discussed. The experiments were supported by the MutPy system for mutation testing of Python programs.},
author = {Derezinska, Anna and Hałas, Konrad},
doi = {10.1109/ICSTW.2014.24},
isbn = {9780769551944},
journal = {Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2014},
keywords = {Python,dynamically typed language,higher order mutation,mutation testing},
mendeley-groups = {VV{\&}T},
pages = {156--164},
title = {{Experimental evaluation of mutation testing approaches to Python programs}},
year = {2014}
}

@article{Li:2015,
abstract = {Mutation testing is an effective testing technique to detect faults and improve code quality. However, few practitioners have adopted mutation testing into practice, which raises several questions: Are tests capable of killing mutants useful? What is the main hindrance to adopting mutation testing in practice? Can practitioners really integrate mutation testing into real-world agile development processes? In this paper, we present two major contributions. First, based on our analysis and knowledge of Ruby, we devised eight new mutation operators for Ruby. Second, we applied mutation testing to an industrial Ruby project at Medidata and reported the lessons learned from the study. We confirmed that mutation-adequate tests are useful and could improve code quality from the perspective of practitioners and found long mutation execution time hinders the agile process. In addition, we used an enterprise-level Amazon cloud-computing technique to reduce the computational cost of running mutants. Considering the availability of a mutation testing tool with our suggested features, we argue that mutation testing can be used in practice.},
author = {Li, Nan and West, Michael and Escalona, Anthony and Durelli, Vinicius H. S.},
doi = {10.1109/ICSTW.2015.7107453},
isbn = {978-1-4799-1885-0},
journal = {2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
keywords = {Computational efficiency,Computers,Conferences,Indexes,Industries,Software,Testing},
mendeley-groups = {VV{\&}T},
number = {i},
pages = {1--6},
title = {{Mutation testing in practice using Ruby}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7107453},
year = {2015}
}

@inproceedings{Delamaro:1996,
abstract = {This technical report presents the main features of Proteum (PROgram Testing Using Mutants), a testing tool that supports Mutation Analysis criterion. Proteum can be configured for testing programs in many procedural programming languages. This guide reports the version 1.1-C that works with the C language on Sun workstations, under OpenWindows environment. Proteum has been developed at the University Sao Paulo (USP), Sao Carlos, SP, Brazil and used in teaching and researching activities at USP and at SERC/Purdue University.},
author = {Delamaro, M{\'{a}}rcio Eduardo and Maldonado, Jos{\'{e}} Carlos},
booktitle = {Proc. of the Conference on Performability in Computing Sys. (PCS 96)},
keywords = {mutation,testing},
pages = {79--95},
title = {{Proteum -- A Tool for the Assessment of Test Adequacy for {\{}C{\}} Programs}},
url = {citeseer.ist.psu.edu/delamaro96proteum.html},
year = {1996}
}


@article{Bottaci:2010,
author = {Bottaci, Leonardo},
doi = {10.1109/ICSTW.2010.56},
file = {:Users/seocam/Documents/Mendeley/2010/Bottaci/Bottaci{\_}2010{\_}Type Sensitive Application of Mutation Operators for Dynamically Typed Programs.pdf:pdf},
isbn = {978-1-4244-6773-0},
journal = {2010 Third International Conference on Software Testing, Verification, and Validation Workshops},
keywords = {-software testing,dynamically,javascript,mutation analysis,typed languages},
mendeley-groups = {VV{\&}T},
pages = {126--131},
title = {{Type Sensitive Application of Mutation Operators for Dynamically Typed Programs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5463640},
year = {2010}
}

@incollection{Bessam:2014,
address = {Cham},
author = {Derezińska, Anna and Hałas, Konrad},
doi = {10.1007/978-3-319-07013-1{\_}15},
editor = {Zamojski, Wojciech and Mazurkiewicz, Jacek and Sugier, Jarosław and Walkowiak, Tomasz and Kacprzyk, Janusz},
file = {:Users/seocam/Documents/Mendeley/2014/Bessam/Bessam{\_}2014{\_}Analysis of Mutation Operators for the Python Language.pdf:pdf},
isbn = {978-3-319-07012-4},
keywords = {asset criticality,cmms,craft-men hours,pm flexibility,preventive maintenance,work priority,workload balancing},
mendeley-groups = {VV{\&}T},
pages = {155--164},
publisher = {Springer International Publishing},
series = {Advances in Intelligent Systems and Computing},
title = {{Analysis of Mutation Operators for the Python Language}},
url = {http://link.springer.com/10.1007/978-3-319-07013-1 http://link.springer.com/10.1007/978-3-319-07013-1{\_}15},
volume = {286},
year = {2014}
}


@article{Frankl:1993,
abstract = {An experiment comparing the effectiveness of the all-uses and all-edges test data adequacy criteria is discussed. The experiment was designed to overcome some of the deficiencies of previous software testing experiments. A large number of test sets was randomly generated for each of nine subject programs with subtle errors. For each test set, the percentages of executable edges and definition-use associations covered were measured, and it was determined whether the test set exposed an error. Hypothesis testing was used to investigate whether all-uses adequate test sets are more likely to expose errors than are all-edges adequate test sets. Logistic regression analysis was used to investigate whether the probability that a test set exposes an error increases as the percentage of definition-use associations or edges covered by it increases. Error exposing ability was shown to be strongly positively correlated to percentage of covered definition-use associations in only four of the nine subjects. Error exposing ability was also shown to be positively correlated to the percentage of covered edges in four different subjects, but the relationship was weaker},
author = {Frankl, P.G. and Weiss, S.N.},
doi = {10.1109/32.238581},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Acoustic testing,Computer science,Data analysis,Error correction,Genetic mutations,Logistics,Performance evaluation,Random number generation,Regression analysis,Software testing,all-edges test data adequacy criteria,all-uses adequate test sets,branch testing,data flow testing,definition-use associations,error exposing ability,errors,executable edges,program testing,regression analysis,software testing experiments},
number = {8},
pages = {774--787},
title = {{An experimental comparison of the effectiveness of branch testing and data flow testing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=238581},
volume = {19},
year = {1993}
}

@article{Li:2009,
abstract = {With recent increased expectations for quality, and the growth of agile processes and test driven development, developers are expected to do more and more effective unit testing. Yet, our knowledge of when to use the various unit level test criteria is incomplete. The paper presents results from a comparison of four unit level software testing criteria. Mutation testing, prime path coverage, edge pair coverage, and all-uses testing were compared on two bases: the number of seeded faults found and the number of tests needed to satisfy the criteria. The comparison used a collection of Java classes taken from various sources and hand-seeded faults. Tests were designed and generated mostly by hand with help from tools that compute test requirements and muJava. The findings are that mutation tests detected more faults and the other three criteria were very similar. The paper also presents a secondary measure, a cost benefit ratio, computed as the number of tests needed to detect each fault. Surprisingly, mutation required the fewest number of tests. The paper also discusses some specific faults that were not found and presents analysis for why not.},
author = {Li, Nan and Praphamontripong, Upsorn and Offutt, Jeff},
doi = {10.1109/ICSTW.2009.30},
isbn = {9780769536712},
journal = {IEEE International Conference on Software Testing, Verification, and Validation Workshops, ICSTW 2009},
pages = {220--229},
title = {{An experimental comparison of four unit test criteria: Mutation, edge-pair, all-uses and prime path coverage}},
year = {2009}
}


@article{Andrews:2005,
abstract = { The empirical assessment of test techniques plays an important role in software testing research. One common practice is to instrument faults, either manually or by using mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. This paper investigates this important question based on a number of programs with comprehensive pools of test cases and known faults. It is concluded that, based on the data available thus far, the use of mutation operators is yielding trustworthy results (generated mutants are similar to real faults). Mutants appear however to be different from hand-seeded faults that seem to be harder to detect than real faults.},
author = {Andrews, J.H. and Briand, L.C. and Labiche, Y.},
doi = {10.1109/ICSE.2005.1553583},
isbn = {1-59593-963-2},
journal = {Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.},
keywords = {experimentation,experimentation is an essential,hand-seeded faults,mutants,part of research in,real faults,software,verification},
pages = {402--411},
title = {{Is mutation an appropriate tool for testing experiments? [software testing]}},
year = {2005}
}

@article{Gopinath:2014,
abstract = {One of the key concerns of developers testing code is how to determine a test suite’s quality – its ability to find faults. The most common approach in industry is to use code cov- erage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by measuring its ability to kill mutants, which are artificially seeded potential faults. Mutation testing is effective but expensive, thus seldom used by practitioners. Determining which criteria best predict mutation kills is therefore critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites — they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that re- sults for evaluation differ from those for suite-comparison: statement coverage (not block, branch, or path) predicts mutation kills best. Categories},
author = {Gopinath, Rahul and Jensen, Carlos and Groce, Alex},
doi = {10.1145/2568225.2568278},
isbn = {9781450327565},
journal = {Proceedings of the 36th International Conference on Software Engineering - ICSE 2014},
keywords = {evaluation of coverage criteria,statistical,test frameworks},
pages = {72--82},
title = {{Code coverage for suite evaluation by developers}},
url = {http://dl.acm.org/citation.cfm?doid=2568225.2568278},
year = {2014}
}
